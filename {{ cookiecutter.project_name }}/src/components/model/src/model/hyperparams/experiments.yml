LGBMRegressor:
  base_trials:
    default:
      objective: "regression"
      learning_rate: 0.1
      num_leaves: 31
      max_depth: -1 
      n_estimators: 100
      min_data_in_leaf: 20
      min_sum_hessian_in_leaf: 1.e-3
      bagging_fraction: 1.0
      reg_alpha: 0.0
      reg_lambda: 0.0
      subsample_freq: 0
      feature_fraction: 1.0
      feature_fraction_bynode: 1.0
      max_delta_step: 0.0
      max_bin: 255
    best_found:
      objective: 'regression_l1'
      learning_rate: 0.09955602146046562
      num_leaves: 62
      max_depth: 19
      n_estimators: 977 
      min_data_in_leaf: 2
      min_sum_hessian_in_leaf: 0.252132217705505
      bagging_fraction: 0.8057087769052775
      reg_alpha: 5.912943827615446
      reg_lambda: 12.19058643459158
      subsample_freq: 0
      feature_fraction: 0.8147072417368565
      feature_fraction_bynode: 0.9892488672802201
      max_delta_step: 2.178503860570072
      max_bin: 405
  grid:
    objective: ["regression", "regression_l1", "huber", "fair", "poisson", "mape", "gamma", "tweedie"]
    learning_rate: [1.e-5, 0.1]
    num_leaves: [2, 64]
    max_depth: [-1, 20]
    n_estimators: [100, 1000]
    min_data_in_leaf: [1, 40]
    min_sum_hessian_in_leaf: [1.e-5, 10]
    bagging_fraction: [0.5, 1.0]
    reg_alpha: [0.0, 100]
    reg_lambda: [0.0, 100]
    subsample_freq: [0, 10]
    feature_fraction: [0.8, 1.0]
    feature_fraction_bynode: [0.8, 1.0]
    max_delta_step: [0.0, 10.0]
    max_bin: [128, 512]
  

XGBRegressor:
  base_trials:
    default:
      objective: "reg:squarederror"
      n_estimators: 1000
      learning_rate: 0.3
      max_depth: 6
      reg_alpha: 0
      reg_lambda: 1
      subsample: 1
  grid:
    n_estimators: [500,2000]
    learning_rate: [1.e-5, 0.3]
    max_depth: [1, 16]
    reg_alpha: [1.e-5, 100]
    reg_lambda: [1.e-5, 100]
    subsample: [0.8, 1]
    colsample_bytree: [0.1, 1]
    colsample_bylevel: [0.1, 1] 
    colsample_bynode: [0.1, 1]
    gamma: [0.01, 1.0]
    min_child_weight: [0.01, 10.0]
    objective: ["reg:squarederror", "reg:squaredlogerror", "reg:pseudohubererror", "reg:absoluteerror", "reg:gamma", "reg:tweedie"]


CatBoostRegressor:
  base_trials:
    default:
      nan_mode: 'Min'
      eval_metric: 'RMSE'
      
      sampling_frequency: 'PerTree'
      leaf_estimation_method: 'Newton'
      random_score_type: 'NormalWithModelSizeDecrease'
      grow_policy: 'SymmetricTree'
      penalties_coefficient: 1
      boosting_type: 'Plain'
      model_shrink_mode: 'Constant'
      feature_border_type: 'GreedyLogSum'
      l2_leaf_reg: 3
      random_strength: 1
      rsm: 1
      subsample: 0.800000011920929
      random_seed: 42
      depth: 6
      posterior_sampling: False
      model_shrink_rate: 0
      loss_function': 'RMSE'
      learning_rate: 0.0637660026550293
      score_function: 'Cosine'
      bootstrap_type: 'MVS'
      max_leaves: 64
    best_found:
      nan_mode: 'Min'
      eval_metric: 'RMSE' 
      sampling_frequency: 'PerTree'
      leaf_estimation_method: 'Newton'
      random_score_type: 'NormalWithModelSizeDecrease'
      grow_policy: 'SymmetricTree'
      penalties_coefficient: 1
      boosting_type: 'Plain'
      model_shrink_mode: 'Constant'
      feature_border_type: 'GreedyLogSum'
      l2_leaf_reg: 0.5092358708053079
      random_strength: 1
      rsm: 1
      subsample: 0.9897972353935439
      random_seed': 42
      depth: 9
      posterior_sampling: False
      model_shrink_rate: 0
      loss_function': 'RMSE'
      learning_rate: 0.09163475026555298
      score_function: 'Cosine'
      bootstrap_type: 'MVS'
      max_leaves: 64

  grid:
    objective: ["RMSE"]
    learning_rate: [1.e-5, 3.e-1]
    grow_policy: ['SymmetricTree']
    random_score_type: ["Gumbel", "NormalWithModelSizeDecrease"]
    penalties_coefficient: [0.5, 2.0]
    depth: [5, 10]
    l2_leaf_reg: [1.e-5, 100]
    subsample: [0.8, 1]
    bootstrap_type: ["MVS", "Bernoulli"]
    leaf_estimation_method: ["Newton", "Gradient"]
    sampling_frequency: ["PerTree", "PerTreeLevel"]
    boosting_type: ["Plain", "Ordered"]
    model_shrink_mode: ["Constant", "Decreasing"]
    feature_border_type: ["GreedyLogSum", "Median", "Uniform", "MinEntropy"]
    # posterior_sampling: [true, false] # TODO: Parse booleans in tuning
    score_function: ["Cosine", "L2"]
    random_strength: [0.5, 2]
    #model_shrink_rate: [0, 0.9]
    rsm: [0.1, 1]